{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header_cell",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction - Data Loading & Initial Exploration\n",
    "\n",
    "**Project Overview**: Predicting customer churn for a telecommunications company\n",
    "\n",
    "**Author**: Muhammad Afnan\n",
    "\n",
    "**Date**: 11 Jun 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "This notebook focuses on:\n",
    "1. Loading the dataset\n",
    "2. Initial data exploration\n",
    "3. Basic data quality assessment\n",
    "4. Understanding data structure and types\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_loading_header",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Note: Update the path according to your local setup\n",
    "DATA_PATH = \"D:\\Machine Learning Projects\\Customer Churn Prediction\\dataset\\Telco-Customer-Churn.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"‚úÖ Dataset loaded successfully\")\n",
    "    print(f\"üìä Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found at {DATA_PATH}\")\n",
    "    print(\"Please update the DATA_PATH variable with the correct path to your dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial_exploration_header",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first_look",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "print(\"üìã First 5 rows of the dataset:\")\n",
    "print(\"=\" * 50)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"üìà Dataset Information:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Number of rows: {df.shape[0]:,}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_types",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and memory usage\n",
    "print(\"üîç Data Types and Memory Usage:\")\n",
    "print(\"=\" * 35)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_quality_header",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing_values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"üîç Missing Values Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"üîç Duplicate Rows Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f\"Percentage of duplicates: {(duplicate_count/len(df))*100:.2f}%\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate rows found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique_values",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for each column\n",
    "print(\"üîç Unique Values Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "unique_values_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Unique Values': [df[col].nunique() for col in df.columns],\n",
    "    'Data Type': df.dtypes\n",
    "})\n",
    "\n",
    "print(unique_values_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_stats_header",
   "metadata": {},
   "source": [
    "## 5. Basic Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"üìä Statistical Summary (Numerical Columns):\")\n",
    "print(\"=\" * 80)\n",
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "categorical_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary for categorical columns\n",
    "print(\"üìä Categorical Columns Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nüî∏ {col}:\")\n",
    "    value_counts = df[col].value_counts()\n",
    "    print(value_counts)\n",
    "    print(f\"Unique values: {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_analysis_header",
   "metadata": {},
   "source": [
    "## 6. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "target_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the target variable (Churn)\n",
    "print(\"üéØ Target Variable Analysis (Churn):\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if 'Churn' in df.columns:\n",
    "    churn_counts = df['Churn'].value_counts()\n",
    "    churn_percentages = df['Churn'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    target_summary = pd.DataFrame({\n",
    "        'Count': churn_counts,\n",
    "        'Percentage': churn_percentages.round(2)\n",
    "    })\n",
    "    \n",
    "    print(target_summary)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Count plot\n",
    "    churn_counts.plot(kind='bar', ax=ax1, color=['skyblue', 'salmon'])\n",
    "    ax1.set_title('Churn Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Churn')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.tick_params(axis='x', rotation=0)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(churn_counts.values):\n",
    "        ax1.text(i, v + 50, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    ax2.pie(churn_counts.values, labels=churn_counts.index, autopct='%1.1f%%', \n",
    "            colors=['skyblue', 'salmon'], startangle=90)\n",
    "    ax2.set_title('Churn Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Class imbalance check\n",
    "    minority_class_pct = min(churn_percentages)\n",
    "    if minority_class_pct < 30:\n",
    "        print(f\"\\n‚ö†Ô∏è Class imbalance detected! Minority class: {minority_class_pct:.1f}%\")\n",
    "        print(\"Consider using sampling techniques during model training.\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Balanced dataset. Minority class: {minority_class_pct:.1f}%\")\n",
    "else:\n",
    "    print(\"‚ùå 'Churn' column not found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6c9f8",
   "metadata": {},
   "source": [
    "## 7. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42404c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean TotalCharges column\n",
    "print(\"üßπ Cleaning TotalCharges column:\")\n",
    "print(\"=\" * 35)\n",
    "l1= []\n",
    "l2 = []\n",
    "for i in (df['TotalCharges']):\n",
    "    length = len(i.split())\n",
    "    l1.append(length)\n",
    "for i in range(len(l1)):\n",
    "    if l1[i] != 1:\n",
    "        l2.append(i)\n",
    "for i in l2:\n",
    "    df.loc[i,'TotalCharges'] = df.loc[(i-1),'TotalCharges']\n",
    "df['TotalCharges'] = df['TotalCharges'].astype(float)\n",
    "print(f\"‚úÖ TotalCharges cleaning completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for processing\n",
    "df_clean = df.copy()\n",
    "print(f\"üìã Working with a copy of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db71135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove CustomerID as it's not useful for prediction\n",
    "print(\"üóëÔ∏è Removing CustomerID column:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "if 'customerID' in df_clean.columns:\n",
    "    print(f\"CustomerID samples: {df_clean['customerID'].head().tolist()}\")\n",
    "    df_clean = df_clean.drop(columns=['customerID'])\n",
    "    print(\"‚úÖ CustomerID column removed\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è CustomerID column not found\")\n",
    "\n",
    "print(f\"New dataset shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a42aa5",
   "metadata": {},
   "source": [
    "## 8. Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5779657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "text_data_features = []\n",
    "for i in list(df_clean.columns):\n",
    "    if i not in list(df.describe().columns):\n",
    "        text_data_features.append(i)\n",
    "print(\"Label Encoder Transformation\")\n",
    "\n",
    "for i in text_data_features:\n",
    "    df_clean[i] = le.fit_transform(df_clean[i])\n",
    "    print(i, ' : ',df_clean[i].unique(),' = ' ,le.inverse_transform(df_clean[i].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['Orange', \"Black\"]\n",
    "churn = df_clean[df_clean['Churn']==1].describe().T\n",
    "not_churn = df_clean[df_clean['Churn']==0].describe().T\n",
    "\n",
    "fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (5,5))\n",
    "plt.subplot(1,2,1)\n",
    "sns.heatmap(churn[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')\n",
    "plt.title('Churned Customers');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.heatmap(not_churn[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f',)\n",
    "plt.title('Not_Churned Customers');\n",
    "fig.tight_layout(pad = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_header",
   "metadata": {},
   "source": [
    "## 7. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Missing Values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate Rows: {df.duplicated().sum()}\")\n",
    "print(f\"Numerical Columns: {len(numerical_cols)}\")\n",
    "print(f\"Categorical Columns: {len(categorical_cols)}\")\n",
    "\n",
    "if 'Churn' in df.columns:\n",
    "    churn_rate = (df['Churn'] == 'Yes').mean() * 100\n",
    "    print(f\"Churn Rate: {churn_rate:.1f}%\")\n",
    "\n",
    "print(\"\\nüìù NEXT STEPS:\")\n",
    "print(\"=\" * 15)\n",
    "print(\"1. ‚úÖ Data Loading & Exploration & Cleaning - COMPLETED\")\n",
    "print(\"2. üîÑ Exploratory Data Analysis (EDA)\")\n",
    "print(\"3. üîÑ Feature Engineering\")\n",
    "\n",
    "# Save the loaded data for next notebook\n",
    "print(\"\\nüíæ Saving processed data for next notebook...\")\n",
    "df.to_csv('Telco-Customer-Churn.csv', index=False)\n",
    "print(\"‚úÖ Data saved to 'Telco-Customer-Churn.csv'\")\n",
    "df_clean.to_csv('Telco-Customer-Churn-Cleaned.csv', index=False)\n",
    "print(\"‚úÖ Data saved to 'Telco-Customer-Churn-Cleaned.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
